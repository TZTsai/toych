import numpy as np
import inspect
import functools
from utils import ProfileOp, ABC, abstractmethod
# from my_utils.utils import interact


class Parameter(np.ndarray):
    """A trainable parameter in a node."""

    grad_lim = 1.0e8  # limit of the magnitude of each element of the gradient
    training = True
    rng = np.random.default_rng()

    def __new__(cls, value=None, *, size=None, mean=0, scale=None,
                dtype=np.float, learnable=True):
        """
        If `value` is given, then it will be converted to a Parameter.
        If `dtype` is the same as that of the given `value`, then a view of
        `value` will be returned, so its data will not be copied.
        However, if `size` is additionally specified, then a new Parameter
        of this size will be created filled with the given `value`.
        
        If `value` is not given, a random Parameter following normal
        distribution will be generated. Additionally, `mean` and `scale`
        of the distribution can be specified.
        
        >>> Parameter([[1,2,3],[4,5,6]])
        >>> Parameter(size=[4, 4], dtype=np.float32, scale=1)
        >>> Parameter(0, size=[5, 5])
        >>> w = Parameter(size=[5, 3])
        >>> w is Parameter(w)
        """
        if value is None:  # random initialization
            if size is None:
                size = 1
            if scale is None:
                length = size[0] if hasattr(size, '__len__') else size
                scale = 1 / np.sqrt(length)  # Xavier initialization
            value = self.rng.normal(size=size, loc=mean, scale=scale)
        else:  # convert the value to an array
            if size is not None:  # fill an array of the given size
                value = np.full(size, value, dtype=dtype)
        param = np.asarray(value, dtype=dtype).view(cls)
        param.learnable = learnable
        param._ctx = None
        param._grad = 0 if learnable else None
        return param

    @property
    def grad(self):
        return self._grad

    @grad.setter
    def grad(self, grad):
        if not self.learnable: return
        elif np.shape(grad) == ():  # a constant gradient
            grad = np.full_like(self, grad)
        elif np.shape(grad) != self.shape:
            raise ValueError('incorrect dimension of gradient')
        self._grad = np.clip(grad, -self.grad_lim, self.grad_lim)

    def zero_grad(self):
        self._grad = 0
        
    def fill(self, value):
        self[:] = np.full_like(self, value)

    @property
    def need_update(self):
        return self.learnable and id(self.grad) != id(0)
    
    def backward(self):
        def bwd(param, visited=set()):
            ctx = param._ctx
            if ctx is None or param in visited: return
            grads = [ctx.grad] if len(ctx.parents) == 1 else ctx.grad
            assert len(grads) == len(ctx.parents)
            for par, grad in zip(ctx.parents, grads):
                par.grad = param.grad @ grad
                visited.add(par)
                bwd(par, visited)
        self.grad = 1  # its gradient wrt itself is constant 1
        bwd(self)

        
class Operation(type):
    """
    A metaclass of Parameter operations.
    Classes generated by this metaclass must implement the `apply` method.
    """
    
    class AbstractOp:
        def __new__(cls, *args, **kwds):
            op = object.__new__(cls)
            op.parents = args
            op.grad = None
            # with ProfileOp(cls.__name__, args):
            ret = op.apply(*args, **kwds)
            if ret.learnable: ret._ctx = op
            return ret
        
        @abstractmethod
        def apply(self, *args, **kwds):
            """Computes the output and stores its gradient in `self.grad`."""
            raise NotImplementedError
        
        def __call__(self, *args, **kwds):
            """Wraps the apply method to process arguments and return value."""
            return Parameter(self.apply(*args, **kwds),
                             learnable=any(p.learnable for p in args))
        
        def __repr__(self):
            return f"{type(self).__name__}({', '.join(map(repr, self.parents))})"

    def __new__(meta, name, bases, ns):
        return type.__new__(meta, name, (meta.AbstractOp,), ns)
        
    def __call__(cls, *args, **kwds):
        op = cls.AbstractOp.__new__(cls, *args, **kwds)
        
        # register the operation to Parameter
        name = cls.__name__.lower()
        setattr(Parameter, name, op)
        
        # operator overloading
        if name in ['add', 'sub', 'mul', 'pow', 'matmul']:
            setattr(Parameter, f"__{name}__", op)
            setattr(Parameter, f"__r{name}__", lambda self, x: op(x, self))
            setattr(Parameter, f"__i{name}__", lambda self, x: self.fill(op(self, x)))
        
        return op
    
    def __repr__(self):
        return self.__name__
